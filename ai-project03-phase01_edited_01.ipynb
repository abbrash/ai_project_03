{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Env():\n",
    "    def __init__(self, length, width, hole_list, wall_list, start_loc, goal_loc):\n",
    "        # define the height and length of the map\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "\n",
    "        self.hole = hole_list\n",
    "        self.wall = wall_list\n",
    "        self.start_loc = start_loc   # [x_start, y_start]\n",
    "        self.goal_loc = goal_loc     # [x_goal, y_goal]\n",
    "\n",
    "        # define the agent's start position\n",
    "        self.x = start_loc[0]\n",
    "        self.y = start_loc[1]\n",
    "\n",
    "        # self.line = \n",
    "\n",
    "\n",
    "    def render(self, frames=50):\n",
    "        # for i in range(self.width):\n",
    "        #     if i == 0: # cliff is in the line 0\n",
    "        #         line = ['S'] + ['x']*(self.length - 2) + ['G'] # 'S':start, 'G':goal, 'x':the cliff\n",
    "        #     else:\n",
    "        #         line = ['.'] * self.length\n",
    "        #     if self.x == i:\n",
    "        #         line[self.y] = 'o' # mark the agent's position as 'o'\n",
    "            \n",
    "        #     print(''.join(line))\n",
    "        # print('\\033['+str(self.width+1)+'A')  # printer go back to top-left \n",
    "        # time.sleep(1.0 / frames)\n",
    "\n",
    "        for i in range(self.length):\n",
    "            for j in range(self.width): \n",
    "\n",
    "                if ([i,j] == [self.x, self.y]):\n",
    "                    line = ['S'] + ['x']*(self.length - 2) + ['G'] # 'S':start, 'G':goal, 'x':the cliff \n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"4 legal actions, 0:up, 1:down, 2:left, 3:right\"\"\"\n",
    "        change = [[0, 1], [0, -1], [-1, 0], [1, 0]]\n",
    "        self.x = min(self.width - 1, max(0, self.x + change[action][0]))\n",
    "        self.y = min(self.length - 1, max(0, self.y + change[action][1]))\n",
    "\n",
    "        states = [self.x, self.y]\n",
    "        reward = -1\n",
    "        terminal = False\n",
    "        if self.x == 0: # if agent is on the cliff line \"SxxxxxT\"\n",
    "            if self.y > 0: # if agent is not on the start position \n",
    "                terminal = True\n",
    "                if self.y != self.length - 1: # if agent falls\n",
    "                    reward = -100\n",
    "        return reward, states, terminal\n",
    "\n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Q_table():\n",
    "    def __init__(self, length, height, actions=4, alpha=0.1, gamma=0.9):\n",
    "        self.table = [0] * actions * length * height # initialize all Q(s,a) to zero\n",
    "        self.actions = actions\n",
    "        self.length = length\n",
    "        self.height = height\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _index(self, a, x, y):\n",
    "        \"\"\"Return the index of Q([x,y], a) in Q_table.\"\"\"\n",
    "        return a * self.height * self.length + x * self.length + y\n",
    "\n",
    "    def _epsilon(self):\n",
    "        return 0.1\n",
    "        # version for better convergence:\n",
    "        # \"\"\"At the beginning epsilon is 0.2, after 300 episodes decades to 0.05, and eventually go to 0.\"\"\"\n",
    "        # return 20. / (num_episode + 100)\n",
    "\n",
    "    def take_action(self, x, y, num_episode):\n",
    "        \"\"\"epsilon-greedy action selection\"\"\"\n",
    "        if random.random() < self._epsilon():\n",
    "            return int(random.random() * 4)\n",
    "        else:\n",
    "            actions_value = [self.table[self._index(a, x, y)] for a in range(self.actions)]\n",
    "            return actions_value.index(max(actions_value))\n",
    "\n",
    "    def max_q(self, x, y):\n",
    "        actions_value = [self.table[self._index(a, x, y)] for a in range(self.actions)]\n",
    "        return max(actions_value)\n",
    "\n",
    "    def update(self, a, s0, s1, r, is_terminated):\n",
    "        # both s0, s1 have the form [x,y]\n",
    "        q_predict = self.table[self._index(a, s0[0], s0[1])]\n",
    "        if not is_terminated:\n",
    "            q_target = r + self.gamma * self.max_q(s1[0], s1[1])\n",
    "        else:\n",
    "            q_target = r\n",
    "        self.table[self._index(a, s0[0], s0[1])] += self.alpha * (q_target - q_predict)\n",
    "\n",
    "\n",
    "def cliff_walk():\n",
    "    env = Env(length=12, width=4)\n",
    "    table = Q_table(length=12, height=4)\n",
    "    num_ep = []\n",
    "    reward_ep = []\n",
    "    for num_episode in range(10):\n",
    "        # within the whole learning process\n",
    "        episodic_reward = 0\n",
    "        is_terminated = False\n",
    "        s0 = [0, 0]\n",
    "\n",
    "        while not is_terminated:\n",
    "            # within one episode\n",
    "            action = table.take_action(s0[0], s0[1], num_episode)\n",
    "            r, s1, is_terminated = env.step(action)\n",
    "            table.update(action, s0, s1, r, is_terminated)\n",
    "            episodic_reward += r\n",
    "            env.render(frames=1)\n",
    "            s0 = s1\n",
    "        if num_episode % 1 == 0:\n",
    "            print(\"Episode: {}, Score: {}\".format(num_episode, episodic_reward))\n",
    "            num_ep.append(num_episode)\n",
    "            reward_ep.append(episodic_reward)\n",
    "            \n",
    "        env.reset()\n",
    "\n",
    "    return num_ep, reward_ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episode, episodic_reward = cliff_walk()\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(num_episode, episodic_reward)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S . . . . . W . . .\n",
      ". . . . . . . . H .\n",
      ". . . . W W W W . .\n",
      ". . . H . W . W . W\n",
      "W . . . . W . . . W\n",
      ". . W . . W . . . W\n",
      ". . W . . . H . . .\n",
      ". . W . . . . . W .\n",
      ". . W . . W . . W .\n",
      ". . W . . W . . W G\n"
     ]
    }
   ],
   "source": [
    "length = 10\n",
    "width = 10 \n",
    "\n",
    "start_loc = [0,0]\n",
    "goal_loc = [9,9] \n",
    "\n",
    "# 3 Cells as holes are defined in the environment\n",
    "hole = [[8,1], \n",
    "        [3,3],\n",
    "        [6,6]]\n",
    "\n",
    "# 23 Cells as wall are defined in the environment\n",
    "wall = [[6,0], \n",
    "        [4,2], [5,2], [6,2], [7,2],\n",
    "        [5,3], [7,3], [9,3],\n",
    "        [0,4], [5,4], [9,4],\n",
    "        [2,5], [5,5], [9,5],\n",
    "        [2,6], \n",
    "        [2,7], [8,7], \n",
    "        [2,8], [5,8], [8,8],\n",
    "        [2,9], [5,9], [8,9]]\n",
    "\n",
    "line = []\n",
    "\n",
    "empty_env = np.full(shape=(length,width), fill_value='.')\n",
    "empty_env[start_loc[0],start_loc[1]] = 'S'\n",
    "empty_env[goal_loc[0],goal_loc[1]] = 'G'\n",
    "\n",
    "for wall_idx in wall: \n",
    "    empty_env[wall_idx[1],wall_idx[0]] = 'W' \n",
    "\n",
    "for hole_idx in hole: \n",
    "    empty_env[hole_idx[1],hole_idx[0]] = 'H'\n",
    "\n",
    "for i in range(empty_env.shape[0]):\n",
    "        for j in range(empty_env.shape[1]):\n",
    "                line.append(empty_env[i,j])\n",
    "        print(*line)\n",
    "        line = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
